% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={\# Clustering Methods for Categorical and Continuous Data: A Comprehensive Guide},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{\# Clustering Methods for Categorical and Continuous Data: A
Comprehensive Guide}
\author{}
\date{2025-03-01}

\begin{document}
\maketitle
\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{3}
\tableofcontents
}
\section{Introduction to Clustering}\label{introduction-to-clustering}

Clustering is a foundational technique in unsupervised machine learning.
It helps us find natural groupings in data without having pre-labeled
categories. Whether we're analyzing customers, genes, images, or survey
responses, clustering lets us identify patterns and structure in our
data. This makes it a critical tool in data science, especially for
exploratory data analysis and pattern discovery.

Why is clustering so useful? Because it simplifies complex datasets.
Imagine trying to make sense of thousands of customer records.
Clustering can help group them based on behavior or preferences, making
it easier to target marketing strategies or identify needs.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Categories of Clustering
Methods}\label{categories-of-clustering-methods}

Clustering methods generally fall into three main categories:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Model-Based Clustering}: Assumes the data is generated by a
  mixture of underlying statistical distributions (like Gaussians or
  multinomials). Examples: Gaussian Mixture Models (GMM), Latent Class
  Analysis (LCA), Dirichlet Process Mixture Models (DPMM).
\item
  \textbf{Hierarchical Clustering}: Builds a tree of clusters either by
  merging (agglomerative) or splitting (divisive) data points. Examples:
  AGNES, DIANA.
\item
  \textbf{Distance-Based Clustering}: Forms clusters based on a chosen
  distance metric. Examples: K-Means, K-Modes, K-Prototypes, DBSCAN.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Clustering Methods for Continuous
Data}\label{clustering-methods-for-continuous-data}

\subsection{Model-Based: Gaussian Mixture Models
(GMM)}\label{model-based-gaussian-mixture-models-gmm}

GMM assumes data is generated from a mix of multiple Gaussian
distributions. Each cluster has its own mean and covariance. The
Expectation-Maximization (EM) algorithm estimates which Gaussian most
likely generated each point. This allows for \textbf{soft clustering},
where a point belongs to multiple clusters with different probabilities.

\begin{itemize}
\tightlist
\item
  \textbf{Strengths}: Models elliptical clusters, handles soft
  assignments, supports statistical criteria like BIC for choosing the
  number of clusters.
\item
  \textbf{Limitations}: Assumes Gaussian shape, sensitive to
  initialization, can overfit.
\item
  \textbf{Evaluation}: BIC (for choosing cluster count), Silhouette
  score (based on likelihood or distance), ARI/NMI if ground truth is
  available.
\end{itemize}

\subsection{Hierarchical: AGNES and
DIANA}\label{hierarchical-agnes-and-diana}

\begin{itemize}
\item
  \textbf{AGNES (Agglomerative)} starts with each point as its own
  cluster and merges them iteratively based on linkage criteria (single,
  complete, average, or Ward's).
\item
  \textbf{DIANA (Divisive)} starts with all data in one cluster and
  splits recursively.
\item
  \textbf{Strengths}: No need to pre-specify the number of clusters,
  provides dendrograms for visualization.
\item
  \textbf{Limitations}: Computationally expensive (O(n\^{}2)), greedy --
  early mistakes can't be corrected.
\item
  \textbf{Evaluation}: Dendrogram inspection, Silhouette score (with
  Euclidean distance).
\end{itemize}

\subsection{Distance-Based: K-Means}\label{distance-based-k-means}

K-means minimizes the within-cluster sum of squared distances. It uses
centroids and Euclidean distance to assign points.

\begin{itemize}
\tightlist
\item
  \textbf{Strengths}: Fast, scalable, interpretable.
\item
  \textbf{Limitations}: Requires specifying K, sensitive to outliers and
  initialization, assumes spherical clusters.
\item
  \textbf{Evaluation}: Elbow method, Silhouette score, Davies-Bouldin
  Index, Calinski-Harabasz Index.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Clustering Methods for Categorical
Data}\label{clustering-methods-for-categorical-data}

\subsection{Model-Based: Latent Class Analysis
(LCA)}\label{model-based-latent-class-analysis-lca}

LCA assumes each data point belongs to one latent class, and given that
class, the categorical attributes are independent.

\begin{itemize}
\tightlist
\item
  \textbf{Strengths}: Handles multivariate categorical data, gives
  probabilities, supports statistical testing.
\item
  \textbf{Limitations}: Assumes conditional independence, slow for large
  data, risk of overfitting.
\item
  \textbf{Evaluation}: BIC (for model selection), ARI/NMI (if ground
  truth exists).
\end{itemize}

\subsection{Model-Based (Bayesian): Dirichlet Process Mixture Models
(DPMM)}\label{model-based-bayesian-dirichlet-process-mixture-models-dpmm}

A Bayesian non-parametric method where the number of clusters is not
fixed and can grow with the data.

\begin{itemize}
\tightlist
\item
  \textbf{Strengths}: Automatically determines the number of clusters,
  flexible, supports soft clustering.
\item
  \textbf{Limitations}: Computationally intensive, requires
  setting/tuning hyperparameters.
\item
  \textbf{Evaluation}: Posterior probabilities, Predictive
  log-likelihood, ARI/NMI for known labels.
\end{itemize}

\subsection{Hierarchical: ROCK and Gower's
Distance}\label{hierarchical-rock-and-gowers-distance}

\begin{itemize}
\item
  \textbf{ROCK} merges clusters based on the number of common neighbors
  above a similarity threshold.
\item
  \textbf{Gower + Hierarchical}: Use Gower's distance (handles mixed
  types) with standard hierarchical clustering.
\item
  \textbf{Strengths}: ROCK is robust to noise; Gower allows mixed-type
  clustering.
\item
  \textbf{Limitations}: Requires threshold tuning (ROCK), slow for large
  datasets, not commonly implemented.
\item
  \textbf{Evaluation}: Silhouette with Gower/Hamming distance, visual
  inspection via dendrogram.
\end{itemize}

\subsection{Distance-Based: K-Modes}\label{distance-based-k-modes}

An extension of K-Means for categorical data. It uses the mode (most
common value) instead of the mean and mismatch count (Hamming distance)
as the distance.

\begin{itemize}
\tightlist
\item
  \textbf{Strengths}: Fast, interpretable, handles large categorical
  datasets.
\item
  \textbf{Limitations}: Needs K upfront, sensitive to initialization,
  can be affected by high-cardinality categories.
\item
  \textbf{Evaluation}: Cost function (total mismatches), Silhouette
  (with Hamming distance), ARI/NMI.
\end{itemize}

\subsection{Distance-Based for Mixed Data:
K-Prototypes}\label{distance-based-for-mixed-data-k-prototypes}

Combines K-Means and K-Modes to handle both numeric and categorical
data. Uses Euclidean distance for numerics, Hamming distance for
categoricals.

\begin{itemize}
\tightlist
\item
  \textbf{Strengths}: Handles mixed data, interpretable clusters.
\item
  \textbf{Limitations}: Requires tuning the weight parameter (gamma),
  needs K upfront.
\item
  \textbf{Evaluation}: Silhouette (with combined distance), ARI/NMI.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Comparison Table}\label{comparison-table}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Algorithm & Category & Suitable Data Type \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
GMM (e.g., Mclust) & Model-Based & Continuous \\
AGNES (Agglomerative) & Hierarchical & Continuous \\
DIANA (Divisive) & Hierarchical & Continuous \\
K-Means & Distance-Based & Continuous \\
LCA & Model-Based & Categorical \\
DPMM & Model-Based & Categorical / Continuous \\
ROCK & Hierarchical & Categorical \\
Gower + Hierarchical & Hierarchical & Mixed / Categorical \\
K-Modes & Distance-Based & Categorical \\
K-Prototypes & Distance-Based & Mixed \\
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Clustering Evaluation
Metrics}\label{clustering-evaluation-metrics}

\subsection{Internal Metrics (No Ground Truth
Needed)}\label{internal-metrics-no-ground-truth-needed}

\begin{itemize}
\tightlist
\item
  \textbf{Silhouette Score}: Measures how well a point fits within its
  cluster. Ranges from -1 (bad) to +1 (good). Works with Euclidean,
  Hamming, or Gower distance.
\item
  \textbf{Dunn Index}: Ratio of minimum inter-cluster distance to
  maximum intra-cluster diameter. Higher is better.
\item
  \textbf{Davies-Bouldin Index (DBI)}: Measures average similarity
  between clusters. Lower is better.
\item
  \textbf{Calinski-Harabasz Index (CH)}: Ratio of between-cluster
  dispersion to within-cluster dispersion. Higher is better.
\item
  \textbf{Elbow Method}: Plot of total within-cluster sum of squares vs
  K. Look for an elbow point.
\item
  \textbf{BIC (Bayesian Information Criterion)}: Used in model-based
  clustering to balance model fit and complexity.
\end{itemize}

\subsection{External Metrics (Need Ground Truth
Labels)}\label{external-metrics-need-ground-truth-labels}

\begin{itemize}
\tightlist
\item
  \textbf{Adjusted Rand Index (ARI)}: Compares cluster assignments with
  true labels. Ranges from -1 to 1.
\item
  \textbf{Normalized Mutual Information (NMI)}: Measures mutual
  dependence between predicted clusters and true labels. Ranges from 0
  to 1.
\item
  \textbf{Purity}: Fraction of dominant class members in each cluster.
  Simple but biased toward more clusters.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Conclusion}\label{conclusion}

Clustering is a powerful tool for uncovering hidden patterns in data.
Different methods are suited to different data types: - Use
\textbf{K-Means}, \textbf{GMM}, or \textbf{Hierarchical clustering} with
Euclidean distance for continuous data. - Use \textbf{K-Modes},
\textbf{LCA}, \textbf{ROCK}, or \textbf{Gower-based methods} for
categorical data. - Use \textbf{K-Prototypes} or \textbf{Gower} for
mixed datasets.

Evaluation is key. Use \textbf{internal metrics} like silhouette or BIC
when ground truth isn't available. Use \textbf{external metrics} like
ARI and NMI when it is. Understanding both your data type and the
assumptions of each algorithm is critical to choosing the right
clustering technique.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}



\end{document}
