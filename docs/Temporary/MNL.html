<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.506">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-03-01">

<title>Sreyashi Bhattacharya - Clustering Methods for Categorical and Continuous Data: A Comprehensive Guide</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Sreyashi Bhattacharya</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> <i class="bi bi-person-circle" role="img">
</i> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../resume.html"> <i class="bi bi-file-earmark-check" role="img">
</i> 
<span class="menu-text">Resume</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../data-analysis.html"> <i class="bi bi-bar-chart" role="img">
</i> 
<span class="menu-text">Data Analysis</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../statistics.html"> <i class="bi bi-calculator" role="img">
</i> 
<span class="menu-text">Statistics</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../RA_post.html"> <i class="bi bi-graph-up" role="img">
</i> 
<span class="menu-text">Research Assistantship</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-to-clustering" id="toc-introduction-to-clustering" class="nav-link active" data-scroll-target="#introduction-to-clustering">Introduction to Clustering</a></li>
  <li><a href="#categories-of-clustering-methods" id="toc-categories-of-clustering-methods" class="nav-link" data-scroll-target="#categories-of-clustering-methods">Categories of Clustering Methods</a></li>
  <li><a href="#clustering-methods-for-continuous-data" id="toc-clustering-methods-for-continuous-data" class="nav-link" data-scroll-target="#clustering-methods-for-continuous-data">Clustering Methods for Continuous Data</a>
  <ul class="collapse">
  <li><a href="#model-based-gaussian-mixture-models-gmm" id="toc-model-based-gaussian-mixture-models-gmm" class="nav-link" data-scroll-target="#model-based-gaussian-mixture-models-gmm">Model-Based: Gaussian Mixture Models (GMM)</a></li>
  <li><a href="#hierarchical-agnes-and-diana" id="toc-hierarchical-agnes-and-diana" class="nav-link" data-scroll-target="#hierarchical-agnes-and-diana">Hierarchical: AGNES and DIANA</a></li>
  <li><a href="#distance-based-k-means" id="toc-distance-based-k-means" class="nav-link" data-scroll-target="#distance-based-k-means">Distance-Based: K-Means</a></li>
  </ul></li>
  <li><a href="#clustering-methods-for-categorical-data" id="toc-clustering-methods-for-categorical-data" class="nav-link" data-scroll-target="#clustering-methods-for-categorical-data">Clustering Methods for Categorical Data</a>
  <ul class="collapse">
  <li><a href="#model-based-latent-class-analysis-lca" id="toc-model-based-latent-class-analysis-lca" class="nav-link" data-scroll-target="#model-based-latent-class-analysis-lca">Model-Based: Latent Class Analysis (LCA)</a></li>
  <li><a href="#model-based-bayesian-dirichlet-process-mixture-models-dpmm" id="toc-model-based-bayesian-dirichlet-process-mixture-models-dpmm" class="nav-link" data-scroll-target="#model-based-bayesian-dirichlet-process-mixture-models-dpmm">Model-Based (Bayesian): Dirichlet Process Mixture Models (DPMM)</a></li>
  <li><a href="#hierarchical-rock-and-gowers-distance" id="toc-hierarchical-rock-and-gowers-distance" class="nav-link" data-scroll-target="#hierarchical-rock-and-gowers-distance">Hierarchical: ROCK and Gower’s Distance</a></li>
  <li><a href="#distance-based-k-modes" id="toc-distance-based-k-modes" class="nav-link" data-scroll-target="#distance-based-k-modes">Distance-Based: K-Modes</a></li>
  <li><a href="#distance-based-for-mixed-data-k-prototypes" id="toc-distance-based-for-mixed-data-k-prototypes" class="nav-link" data-scroll-target="#distance-based-for-mixed-data-k-prototypes">Distance-Based for Mixed Data: K-Prototypes</a></li>
  </ul></li>
  <li><a href="#comparison-table" id="toc-comparison-table" class="nav-link" data-scroll-target="#comparison-table">Comparison Table</a></li>
  <li><a href="#clustering-evaluation-metrics" id="toc-clustering-evaluation-metrics" class="nav-link" data-scroll-target="#clustering-evaluation-metrics">Clustering Evaluation Metrics</a>
  <ul class="collapse">
  <li><a href="#internal-metrics-no-ground-truth-needed" id="toc-internal-metrics-no-ground-truth-needed" class="nav-link" data-scroll-target="#internal-metrics-no-ground-truth-needed">Internal Metrics (No Ground Truth Needed)</a></li>
  <li><a href="#external-metrics-need-ground-truth-labels" id="toc-external-metrics-need-ground-truth-labels" class="nav-link" data-scroll-target="#external-metrics-need-ground-truth-labels">External Metrics (Need Ground Truth Labels)</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Clustering Methods for Categorical and Continuous Data: A Comprehensive Guide</h1>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">March 1, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="introduction-to-clustering" class="level2">
<h2 class="anchored" data-anchor-id="introduction-to-clustering">Introduction to Clustering</h2>
<p>Clustering is a foundational technique in unsupervised machine learning. It helps us find natural groupings in data without having pre-labeled categories. Whether we’re analyzing customers, genes, images, or survey responses, clustering lets us identify patterns and structure in our data. This makes it a critical tool in data science, especially for exploratory data analysis and pattern discovery.</p>
<p>Why is clustering so useful? Because it simplifies complex datasets. Imagine trying to make sense of thousands of customer records. Clustering can help group them based on behavior or preferences, making it easier to target marketing strategies or identify needs.</p>
<hr>
</section>
<section id="categories-of-clustering-methods" class="level2">
<h2 class="anchored" data-anchor-id="categories-of-clustering-methods">Categories of Clustering Methods</h2>
<p>Clustering methods generally fall into three main categories:</p>
<ol type="1">
<li><strong>Model-Based Clustering</strong>: Assumes the data is generated by a mixture of underlying statistical distributions (like Gaussians or multinomials). Examples: Gaussian Mixture Models (GMM), Latent Class Analysis (LCA), Dirichlet Process Mixture Models (DPMM).</li>
<li><strong>Hierarchical Clustering</strong>: Builds a tree of clusters either by merging (agglomerative) or splitting (divisive) data points. Examples: AGNES, DIANA.</li>
<li><strong>Distance-Based Clustering</strong>: Forms clusters based on a chosen distance metric. Examples: K-Means, K-Modes, K-Prototypes, DBSCAN.</li>
</ol>
<hr>
</section>
<section id="clustering-methods-for-continuous-data" class="level2">
<h2 class="anchored" data-anchor-id="clustering-methods-for-continuous-data">Clustering Methods for Continuous Data</h2>
<p>When your dataset is made up of numerical values- like age, income, temperature, coordinates, or test scores- you’re working with continuous data.</p>
<p>The good news? -Most classic clustering algorithms are designed for continuous data! Why? Because it’s easy to calculate distances between numerical points using measures like Euclidean distance (the straight-line distance between two points).</p>
<p>Let’s break down the main types of clustering that work well with continuous data:</p>
<section id="model-based-gaussian-mixture-models-gmm" class="level3">
<h3 class="anchored" data-anchor-id="model-based-gaussian-mixture-models-gmm">Model-Based: Gaussian Mixture Models (GMM)</h3>
<p>Imagine your data comes from a mix of several bell curves (aka Gaussian distributions). Each cluster is one of those bell curves, and the overall dataset is their mixture.</p>
<p>GMM says:</p>
<p><em>“Let’s model the data using a bunch of Gaussians and figure out which point most likely came from which one.”</em></p>
<p>This is called a probabilistic model-instead of assigning each point to one cluster definitively, it gives probabilities (soft assignments).</p>
<section id="how-it-works" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works">How it works:</h4>
<ul>
<li>Starts with a guess about how many Gaussians (clusters) there are.</li>
<li>Uses an algorithm called <strong>EM (Expectation-Maximization)</strong> to:
<ul>
<li>Estimate the probability each point came from each cluster.</li>
<li>Update the cluster shapes and centers based on those probabilities.</li>
</ul></li>
<li>Repeats until everything stabilizes.</li>
</ul>
<p>GMM assumes data is generated from a mix of multiple Gaussian distributions. Each cluster has its own mean and covariance. The Expectation-Maximization (EM) algorithm estimates which Gaussian most likely generated each point. This allows for <strong>soft clustering</strong>, where a point belongs to multiple clusters with different probabilities.</p>
<ul>
<li><strong>Strengths</strong>: Models elliptical clusters, handles soft assignments, supports statistical criteria like BIC for choosing the number of clusters.</li>
<li><strong>Limitations</strong>: Assumes Gaussian shape, sensitive to initialization, can overfit.</li>
<li><strong>Evaluation</strong>: BIC (for choosing cluster count), Silhouette score (based on likelihood or distance), ARI/NMI if ground truth is available.</li>
</ul>
<div id="a96a8bb9" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.mixture <span class="im">import</span> GaussianMixture</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_blobs</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>X, _ <span class="op">=</span> make_blobs(n_samples<span class="op">=</span><span class="dv">300</span>, centers<span class="op">=</span><span class="dv">4</span>, cluster_std<span class="op">=</span><span class="fl">0.6</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>gmm <span class="op">=</span> GaussianMixture(n_components<span class="op">=</span><span class="dv">4</span>).fit(X)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> gmm.predict(X)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> gmm.predict_proba(X)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>probs.<span class="bu">max</span>(axis<span class="op">=</span><span class="dv">1</span>), cmap<span class="op">=</span><span class="st">'coolwarm'</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'GMM Clustering (Soft Assignments)'</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>plt.colorbar(label<span class="op">=</span><span class="st">'Cluster Confidence'</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="MNL_files/figure-html/cell-2-output-1.png" width="556" height="431" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="hierarchical-agnes-and-diana" class="level3">
<h3 class="anchored" data-anchor-id="hierarchical-agnes-and-diana">Hierarchical: AGNES and DIANA</h3>
<p>Hierarchical clustering doesn’t just give you clusters- it builds a tree (called a dendrogram) showing how clusters form or split. You don’t have to decide on the number of clusters up front!</p>
<p>There are two main styles:</p>
<section id="agnes-agglomerative-nesting" class="level4">
<h4 class="anchored" data-anchor-id="agnes-agglomerative-nesting">AGNES (Agglomerative Nesting)</h4>
<p><em>Bottom-up approach.</em></p>
<p>-Start with each point as its own cluster.</p>
<p>-Merge the two closest clusters, step by step, until everything is one big cluster.</p>
<p>-Result: a dendrogram you can “cut” at any level to get your desired number of clusters.</p>
</section>
<section id="diana-divisive-analysis" class="level4">
<h4 class="anchored" data-anchor-id="diana-divisive-analysis">DIANA (Divisive Analysis)</h4>
<p><em>Top-down approach.</em></p>
<p>-Start with all points in one cluster.</p>
<p>-Split the cluster that’s most different inside itself.</p>
<p>-Keep splitting until each point is alone (or you stop earlier).</p>
<p><em>Single linkage:</em> join the closest pair of points. <em>Complete linkage:</em> join based on the furthest points. <em>Ward’s method:</em> join clusters to minimize variance (often works best for compact clusters).</p>
<ul>
<li><p><strong>AGNES (Agglomerative)</strong> starts with each point as its own cluster and merges them iteratively based on linkage criteria (single, complete, average, or Ward’s).</p></li>
<li><p><strong>DIANA (Divisive)</strong> starts with all data in one cluster and splits recursively.</p></li>
<li><p><strong>Strengths</strong>: No need to pre-specify the number of clusters, provides dendrograms for visualization.</p></li>
<li><p><strong>Limitations</strong>: Computationally expensive (O(n^2)), greedy – early mistakes can’t be corrected.</p></li>
<li><p><strong>Evaluation</strong>: Dendrogram inspection, Silhouette score (with Euclidean distance).</p></li>
</ul>
</section>
</section>
<section id="distance-based-k-means" class="level3">
<h3 class="anchored" data-anchor-id="distance-based-k-means">Distance-Based: K-Means</h3>
<p>This is one of the most famous and widely used clustering methods.</p>
<p>K-means says:</p>
<p><em>“Let’s group the data into K clusters so that the points in each group are close to each other (based on average distance from the center).”</em></p>
<section id="how-it-works-1" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-1">How it works:</h4>
<p>-Pick K cluster centers (called centroids) to start- can be random. -Assign each data point to the nearest centroid. -Recalculate centroids as the average of all points assigned to them. -Repeat steps 2–3 until nothing changes.</p>
<p>It’s like playing tug-of-war- points shift between clusters as centroids move, until everyone’s happy.</p>
<p>K-means minimizes the within-cluster sum of squared distances. It uses centroids and Euclidean distance to assign points.</p>
<ul>
<li><strong>Strengths</strong>: Fast, scalable, interpretable.</li>
<li><strong>Limitations</strong>: Requires specifying K, sensitive to outliers and initialization, assumes spherical clusters.</li>
<li><strong>Evaluation</strong>: Elbow method, Silhouette score, Davies-Bouldin Index, Calinski-Harabasz Index.</li>
</ul>
<div id="37974a5b" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> silhouette_score</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>y_kmeans <span class="op">=</span> kmeans.fit_predict(X)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y_kmeans, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>plt.scatter(kmeans.cluster_centers_[:, <span class="dv">0</span>], kmeans.cluster_centers_[:, <span class="dv">1</span>], s<span class="op">=</span><span class="dv">300</span>, c<span class="op">=</span><span class="st">'red'</span>, marker<span class="op">=</span><span class="st">'X'</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'K-Means Clustering'</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Silhouette Score:"</span>, silhouette_score(X, y_kmeans))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:

The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="MNL_files/figure-html/cell-3-output-2.png" width="558" height="431" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Silhouette Score: 0.6819938690643478</code></pre>
</div>
</div>
<hr>
</section>
</section>
</section>
<section id="clustering-methods-for-categorical-data" class="level2">
<h2 class="anchored" data-anchor-id="clustering-methods-for-categorical-data">Clustering Methods for Categorical Data</h2>
<p>Clustering categorical data is a bit trickier than clustering numerical data. Why? Because many clustering algorithms (like k-means) rely on computing distances between data points—using things like Euclidean distance, which only make sense for numbers.</p>
<p>But categorical data—like “red”, “blue”, “yes”, “no”, “urban”, “rural”—can’t be directly averaged or subtracted. So we need methods that understand how to measure similarity without relying on numbers.</p>
<section id="model-based-latent-class-analysis-lca" class="level3">
<h3 class="anchored" data-anchor-id="model-based-latent-class-analysis-lca">Model-Based: Latent Class Analysis (LCA)</h3>
<p>What is it? Latent Class Analysis is like saying: “Let’s assume people (or data points) belong to hidden groups (called latent classes), and their answers to survey questions (or their attribute values) depend on which group they belong to.”</p>
<p>For example, in a survey:</p>
<p>People in Class A might often answer “Yes” to Q1 and Q2, and “No” to Q3.</p>
<p>People in Class B might answer the opposite.</p>
<p><em>How it works:</em> LCA uses probabilities to model this. It says:</p>
<p>Each class has a profile (e.g., 80% chance of answering “Yes” to Q1).</p>
<p>Each person belongs to a class with a certain probability. Then it fits the best model using an algorithm called EM (Expectation Maximization).</p>
<ul>
<li><strong>Strengths</strong>: Handles multivariate categorical data, gives probabilities, supports statistical testing.</li>
<li><strong>Limitations</strong>: Assumes conditional independence, slow for large data, risk of overfitting.</li>
<li><strong>Evaluation</strong>: BIC (for model selection), ARI/NMI (if ground truth exists).</li>
</ul>
</section>
<section id="model-based-bayesian-dirichlet-process-mixture-models-dpmm" class="level3">
<h3 class="anchored" data-anchor-id="model-based-bayesian-dirichlet-process-mixture-models-dpmm">Model-Based (Bayesian): Dirichlet Process Mixture Models (DPMM)</h3>
<p>This is like LCA’s more flexible, Bayesian cousin. Instead of deciding beforehand how many clusters you want (like “K = 3”), it lets the data tell you!</p>
<p><em>The big idea:</em></p>
<p>Imagine a restaurant where each new customer either joins an existing table (cluster) or starts a new one.</p>
<p>Whether they start a new table depends on how many people are already sitting and a concentration parameter (α).</p>
<p>This is called the Chinese Restaurant Process, and it’s the intuition behind DPMM.</p>
<ul>
<li><strong>Strengths</strong>: Automatically determines the number of clusters, flexible, supports soft clustering.</li>
<li><strong>Limitations</strong>: Computationally intensive, requires setting/tuning hyperparameters.</li>
<li><strong>Evaluation</strong>: Posterior probabilities, Predictive log-likelihood, ARI/NMI for known labels.</li>
</ul>
<div id="955c89be" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.mixture <span class="im">import</span> BayesianGaussianMixture</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_blobs</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit DPMM (upper bound on number of components, but fewer are used)</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>dpgmm <span class="op">=</span> BayesianGaussianMixture(</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    n_components<span class="op">=</span><span class="dv">10</span>,       <span class="co"># maximum number of components</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    covariance_type<span class="op">=</span><span class="st">'full'</span>,</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    weight_concentration_prior_type<span class="op">=</span><span class="st">'dirichlet_process'</span>,</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    weight_concentration_prior<span class="op">=</span><span class="fl">0.5</span>,  <span class="co"># lower value → more clusters</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>dpgmm.fit(X)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> dpgmm.predict(X)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> dpgmm.predict_proba(X)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot results</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>labels, cmap<span class="op">=</span><span class="st">'Set2'</span>, s<span class="op">=</span><span class="dv">40</span>)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Dirichlet Process Gaussian Mixture Model (DPMM)'</span>)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="MNL_files/figure-html/cell-4-output-1.png" width="558" height="431" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="whats-happening" class="level4">
<h4 class="anchored" data-anchor-id="whats-happening">What’s Happening?</h4>
<ul>
<li><p>n_components=10: This sets the maximum number of clusters-not the actual number.</p></li>
<li><p>weight_concentration_prior: Lower values allow more small clusters. It’s like tuning α in the Chinese Restaurant Process.</p></li>
</ul>
</section>
</section>
<section id="hierarchical-rock-and-gowers-distance" class="level3">
<h3 class="anchored" data-anchor-id="hierarchical-rock-and-gowers-distance">Hierarchical: ROCK and Gower’s Distance</h3>
<p>When clustering categorical data, we can’t use standard distances like Euclidean. So we need creative alternatives.</p>
<p>ROCK (Robust Clustering using Links) What makes ROCK special? It’s designed specifically for categorical or binary data. Instead of relying just on similarity between two points, it looks at how many common neighbors they have.</p>
<p>Imagine:</p>
<p>Person A is similar to B and C.</p>
<p>Person D is also similar to B and C. Then A and D should probably be in the same group—even if they’re not directly that similar!</p>
<p>It’s like friend-of-a-friend logic. This method merges clusters that are strongly interconnected.</p>
<ul>
<li><strong>ROCK</strong> merges clusters based on the number of common neighbors above a similarity threshold.</li>
</ul>
<p>What is Gower’s Distance? It’s a way to calculate distance that works on mixed data- numerical, categorical, and even ordinal.</p>
<p>For each variable:</p>
<section id="if-its-numeric-take-the-normalized-difference." class="level5">
<h5 class="anchored" data-anchor-id="if-its-numeric-take-the-normalized-difference.">If it’s numeric: take the normalized difference.</h5>
</section>
<section id="if-its-categorical-0-if-they-match-1-if-they-dont." class="level5">
<h5 class="anchored" data-anchor-id="if-its-categorical-0-if-they-match-1-if-they-dont.">If it’s categorical: 0 if they match, 1 if they don’t.</h5>
</section>
<section id="then-average-the-distances.-this-gives-a-similarity-that-respects-data-types" class="level5">
<h5 class="anchored" data-anchor-id="then-average-the-distances.-this-gives-a-similarity-that-respects-data-types">Then, average the distances. This gives a similarity that respects data types!</h5>
<p>How it helps: Once you compute the distance matrix using Gower, you can use any hierarchical clustering method, like AGNES, and plot a dendrogram.</p>
<ul>
<li><p><strong>Gower + Hierarchical</strong>: Use Gower’s distance (handles mixed types) with standard hierarchical clustering.</p></li>
<li><p><strong>Strengths</strong>: ROCK is robust to noise; Gower allows mixed-type clustering.</p></li>
<li><p><strong>Limitations</strong>: Requires threshold tuning (ROCK), slow for large datasets, not commonly implemented.</p></li>
<li><p><strong>Evaluation</strong>: Silhouette with Gower/Hamming distance, visual inspection via dendrogram.</p></li>
</ul>
</section>
</section>
<section id="distance-based-k-modes" class="level3">
<h3 class="anchored" data-anchor-id="distance-based-k-modes">Distance-Based: K-Modes</h3>
<p>An extension of K-Means for categorical data. It uses the mode (most common value) instead of the mean and mismatch count (Hamming distance) as the distance. Why not k-means? Because k-means computes centroids using averages, which doesn’t work for categories like “dog” or “banana.”</p>
<section id="k-modes-to-the-rescue" class="level4">
<h4 class="anchored" data-anchor-id="k-modes-to-the-rescue">K-Modes to the rescue:</h4>
<p>It uses the mode (most frequent category) instead of the mean.</p>
<p>It measures distance by counting how many categories don’t match (like Hamming distance).</p>
<p>Example: If A = [“red”, “large”, “yes”] and B = [“blue”, “large”, “no”] Then they differ in 2 out of 3 features → distance = 2</p>
<ul>
<li><strong>Strengths</strong>: Fast, interpretable, handles large categorical datasets.</li>
<li><strong>Limitations</strong>: Needs K upfront, sensitive to initialization, can be affected by high-cardinality categories.</li>
<li><strong>Evaluation</strong>: Cost function (total mismatches), Silhouette (with Hamming distance), ARI/NMI.</li>
</ul>
</section>
</section>
<section id="distance-based-for-mixed-data-k-prototypes" class="level3">
<h3 class="anchored" data-anchor-id="distance-based-for-mixed-data-k-prototypes">Distance-Based for Mixed Data: K-Prototypes</h3>
<p>The real world isn’t just numbers or categories—it’s both. Enter K-Prototypes, which combines:</p>
<p>K-means (for numerical features)</p>
<p>K-modes (for categorical features)</p>
<p>How it works:</p>
<p>Calculates numerical distance (Euclidean) and categorical mismatch.</p>
<p>Adds them together using a weight factor (γ) to balance them.</p>
<p>Combines K-Means and K-Modes to handle both numeric and categorical data. Uses Euclidean distance for numerics, Hamming distance for categoricals.</p>
<ul>
<li><strong>Strengths</strong>: Handles mixed data, interpretable clusters.</li>
<li><strong>Limitations</strong>: Requires tuning the weight parameter (gamma), needs K upfront.</li>
<li><strong>Evaluation</strong>: Silhouette (with combined distance), ARI/NMI.</li>
</ul>
<hr>
</section>
</section>
<section id="comparison-table" class="level2">
<h2 class="anchored" data-anchor-id="comparison-table">Comparison Table</h2>
<table class="table">
<thead>
<tr class="header">
<th>Algorithm</th>
<th>Category</th>
<th>Suitable Data Type</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GMM (e.g., Mclust)</td>
<td>Model-Based</td>
<td>Continuous</td>
</tr>
<tr class="even">
<td>AGNES (Agglomerative)</td>
<td>Hierarchical</td>
<td>Continuous</td>
</tr>
<tr class="odd">
<td>DIANA (Divisive)</td>
<td>Hierarchical</td>
<td>Continuous</td>
</tr>
<tr class="even">
<td>K-Means</td>
<td>Distance-Based</td>
<td>Continuous</td>
</tr>
<tr class="odd">
<td>LCA</td>
<td>Model-Based</td>
<td>Categorical</td>
</tr>
<tr class="even">
<td>DPMM</td>
<td>Model-Based</td>
<td>Categorical / Continuous</td>
</tr>
<tr class="odd">
<td>ROCK</td>
<td>Hierarchical</td>
<td>Categorical</td>
</tr>
<tr class="even">
<td>Gower + Hierarchical</td>
<td>Hierarchical</td>
<td>Mixed / Categorical</td>
</tr>
<tr class="odd">
<td>K-Modes</td>
<td>Distance-Based</td>
<td>Categorical</td>
</tr>
<tr class="even">
<td>K-Prototypes</td>
<td>Distance-Based</td>
<td>Mixed</td>
</tr>
</tbody>
</table>
<hr>
</section>
<section id="clustering-evaluation-metrics" class="level2">
<h2 class="anchored" data-anchor-id="clustering-evaluation-metrics">Clustering Evaluation Metrics</h2>
<section id="internal-metrics-no-ground-truth-needed" class="level3">
<h3 class="anchored" data-anchor-id="internal-metrics-no-ground-truth-needed">Internal Metrics (No Ground Truth Needed)</h3>
<ul>
<li><strong>Silhouette Score</strong>: Measures how well a point fits within its cluster. Ranges from -1 (bad) to +1 (good). Works with Euclidean, Hamming, or Gower distance.</li>
<li><strong>Dunn Index</strong>: Ratio of minimum inter-cluster distance to maximum intra-cluster diameter. Higher is better.</li>
<li><strong>Davies-Bouldin Index (DBI)</strong>: Measures average similarity between clusters. Lower is better.</li>
<li><strong>Calinski-Harabasz Index (CH)</strong>: Ratio of between-cluster dispersion to within-cluster dispersion. Higher is better.</li>
<li><strong>Elbow Method</strong>: Plot of total within-cluster sum of squares vs K. Look for an elbow point.</li>
<li><strong>BIC (Bayesian Information Criterion)</strong>: Used in model-based clustering to balance model fit and complexity.</li>
</ul>
</section>
<section id="external-metrics-need-ground-truth-labels" class="level3">
<h3 class="anchored" data-anchor-id="external-metrics-need-ground-truth-labels">External Metrics (Need Ground Truth Labels)</h3>
<ul>
<li><strong>Adjusted Rand Index (ARI)</strong>: Compares cluster assignments with true labels. Ranges from -1 to 1.</li>
<li><strong>Normalized Mutual Information (NMI)</strong>: Measures mutual dependence between predicted clusters and true labels. Ranges from 0 to 1.</li>
<li><strong>Purity</strong>: Fraction of dominant class members in each cluster. Simple but biased toward more clusters.</li>
</ul>
<hr>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Clustering is a powerful tool for uncovering hidden patterns in data. Different methods are suited to different data types: - Use <strong>K-Means</strong>, <strong>GMM</strong>, or <strong>Hierarchical clustering</strong> with Euclidean distance for continuous data. - Use <strong>K-Modes</strong>, <strong>LCA</strong>, <strong>ROCK</strong>, or <strong>Gower-based methods</strong> for categorical data. - Use <strong>K-Prototypes</strong> or <strong>Gower</strong> for mixed datasets.</p>
<p>Evaluation is key. Use <strong>internal metrics</strong> like silhouette or BIC when ground truth isn’t available. Use <strong>external metrics</strong> like ARI and NMI when it is. Understanding both your data type and the assumptions of each algorithm is critical to choosing the right clustering technique.</p>
<hr>



</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    const typesetMath = (el) => {
      if (window.MathJax) {
        // MathJax Typeset
        window.MathJax.typeset([el]);
      } else if (window.katex) {
        // KaTeX Render
        var mathElements = el.getElementsByClassName("math");
        var macros = [];
        for (var i = 0; i < mathElements.length; i++) {
          var texText = mathElements[i].firstChild;
          if (mathElements[i].tagName == "SPAN") {
            window.katex.render(texText.data, mathElements[i], {
              displayMode: mathElements[i].classList.contains('display'),
              throwOnError: false,
              macros: macros,
              fleqn: false
            });
          }
        }
      }
    }
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        typesetMath(container);
        return container.innerHTML
      } else {
        typesetMath(note);
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      typesetMath(note);
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>