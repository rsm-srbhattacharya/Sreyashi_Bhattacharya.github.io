{
  "hash": "b607f88ce3689197813627af6c46cd45",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Clustering Methods for Categorical and Continuous Data: A Comprehensive Guide\"\ndate: \"March 2025\"\n---\n\n## Introduction to Clustering\nClustering is a foundational technique in unsupervised machine learning. It helps us find natural groupings in data without having pre-labeled categories. Whether we’re analyzing customers, genes, images, or survey responses, clustering lets us identify patterns and structure in our data. This makes it a critical tool in data science, especially for exploratory data analysis and pattern discovery.\n\nWhy is clustering so useful? Because it simplifies complex datasets. Imagine trying to make sense of thousands of customer records. Clustering can help group them based on behavior or preferences, making it easier to target marketing strategies or identify needs.\n\n---\n\n## Categories of Clustering Methods\nClustering methods generally fall into three main categories:\n\n1. **Model-Based Clustering**: Assumes the data is generated by a mixture of underlying statistical distributions (like Gaussians or multinomials). Examples: Gaussian Mixture Models (GMM), Latent Class Analysis (LCA), Dirichlet Process Mixture Models (DPMM).\n2. **Hierarchical Clustering**: Builds a tree of clusters either by merging (agglomerative) or splitting (divisive) data points. Examples: AGNES, DIANA.\n3. **Distance-Based Clustering**: Forms clusters based on a chosen distance metric. Examples: K-Means, K-Modes, K-Prototypes, DBSCAN.\n\n---\n\n## Clustering Methods for Continuous Data\nWhen your dataset is made up of numerical values- like age, income, temperature, coordinates, or test scores- you’re working with continuous data.\n\nThe good news?\n-Most classic clustering algorithms are designed for continuous data! Why? Because it’s easy to calculate distances between numerical points using measures like Euclidean distance (the straight-line distance between two points).\n\nLet’s break down the main types of clustering that work well with continuous data:\n\n\n### Model-Based: Gaussian Mixture Models (GMM)\nImagine your data comes from a mix of several bell curves (aka Gaussian distributions). Each cluster is one of those bell curves, and the overall dataset is their mixture.\n\nGMM says:\n\n*\"Let’s model the data using a bunch of Gaussians and figure out which point most likely came from which one.\"*\n\nThis is called a probabilistic model-instead of assigning each point to one cluster definitively, it gives probabilities (soft assignments).\n\n#### How it works:\n- Starts with a guess about how many Gaussians (clusters) there are.\n- Uses an algorithm called **EM (Expectation-Maximization)** to:\n  - Estimate the probability each point came from each cluster.\n  - Update the cluster shapes and centers based on those probabilities.\n- Repeats until everything stabilizes.\n\nGMM assumes data is generated from a mix of multiple Gaussian distributions. Each cluster has its own mean and covariance. The Expectation-Maximization (EM) algorithm estimates which Gaussian most likely generated each point. This allows for **soft clustering**, where a point belongs to multiple clusters with different probabilities.\n\n- **Strengths**: Models elliptical clusters, handles soft assignments, supports statistical criteria like BIC for choosing the number of clusters.\n- **Limitations**: Assumes Gaussian shape, sensitive to initialization, can overfit.\n- **Evaluation**: BIC (for choosing cluster count), Silhouette score (based on likelihood or distance), ARI/NMI if ground truth is available.\n\n::: {#a96a8bb9 .cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn.mixture import GaussianMixture\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\n\nX, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.6, random_state=0)\ngmm = GaussianMixture(n_components=4).fit(X)\nlabels = gmm.predict(X)\nprobs = gmm.predict_proba(X)\n\nplt.scatter(X[:, 0], X[:, 1], c=probs.max(axis=1), cmap='coolwarm')\nplt.title('GMM Clustering (Soft Assignments)')\nplt.colorbar(label='Cluster Confidence')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](MNL_files/figure-html/cell-2-output-1.png){width=556 height=431}\n:::\n:::\n\n\n### Hierarchical: AGNES and DIANA\nHierarchical clustering doesn’t just give you clusters- it builds a tree (called a dendrogram) showing how clusters form or split. You don’t have to decide on the number of clusters up front!\n\nThere are two main styles:\n\n#### AGNES (Agglomerative Nesting)\n\n*Bottom-up approach.*\n\n-Start with each point as its own cluster.\n\n-Merge the two closest clusters, step by step, until everything is one big cluster.\n\n-Result: a dendrogram you can “cut” at any level to get your desired number of clusters.\n\n#### DIANA (Divisive Analysis)\n\n*Top-down approach.*\n\n-Start with all points in one cluster.\n\n-Split the cluster that’s most different inside itself.\n\n-Keep splitting until each point is alone (or you stop earlier).\n\n*Single linkage:* join the closest pair of points.\n*Complete linkage:* join based on the furthest points.\n*Ward’s method:* join clusters to minimize variance (often works best for compact clusters).\n\n- **AGNES (Agglomerative)** starts with each point as its own cluster and merges them iteratively based on linkage criteria (single, complete, average, or Ward’s).\n- **DIANA (Divisive)** starts with all data in one cluster and splits recursively.\n\n- **Strengths**: No need to pre-specify the number of clusters, provides dendrograms for visualization.\n- **Limitations**: Computationally expensive (O(n^2)), greedy – early mistakes can't be corrected.\n- **Evaluation**: Dendrogram inspection, Silhouette score (with Euclidean distance).\n\n### Distance-Based: K-Means\nThis is one of the most famous and widely used clustering methods.\n\nK-means says:\n\n*\"Let’s group the data into K clusters so that the points in each group are close to each other (based on average distance from the center).\"*\n\n#### How it works:\n\n-Pick K cluster centers (called centroids) to start- can be random.\n-Assign each data point to the nearest centroid.\n-Recalculate centroids as the average of all points assigned to them.\n-Repeat steps 2–3 until nothing changes.\n\nIt’s like playing tug-of-war- points shift between clusters as centroids move, until everyone’s happy.\n\nK-means minimizes the within-cluster sum of squared distances. It uses centroids and Euclidean distance to assign points.\n\n- **Strengths**: Fast, scalable, interpretable.\n- **Limitations**: Requires specifying K, sensitive to outliers and initialization, assumes spherical clusters.\n- **Evaluation**: Elbow method, Silhouette score, Davies-Bouldin Index, Calinski-Harabasz Index.\n\n::: {#37974a5b .cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nkmeans = KMeans(n_clusters=4)\ny_kmeans = kmeans.fit_predict(X)\n\nplt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red', marker='X')\nplt.title('K-Means Clustering')\nplt.show()\n\nprint(\"Silhouette Score:\", silhouette_score(X, y_kmeans))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](MNL_files/figure-html/cell-3-output-2.png){width=558 height=431}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nSilhouette Score: 0.6819938690643478\n```\n:::\n:::\n\n\n---\n\n## Clustering Methods for Categorical Data\nClustering categorical data is a bit trickier than clustering numerical data. Why? Because many clustering algorithms (like k-means) rely on computing distances between data points—using things like Euclidean distance, which only make sense for numbers.\n\nBut categorical data—like \"red\", \"blue\", \"yes\", \"no\", \"urban\", \"rural\"—can’t be directly averaged or subtracted. So we need methods that understand how to measure similarity without relying on numbers.\n\n### Model-Based: Latent Class Analysis (LCA)\nWhat is it?\nLatent Class Analysis is like saying: “Let’s assume people (or data points) belong to hidden groups (called latent classes), and their answers to survey questions (or their attribute values) depend on which group they belong to.”\n\nFor example, in a survey:\n\nPeople in Class A might often answer “Yes” to Q1 and Q2, and “No” to Q3.\n\nPeople in Class B might answer the opposite.\n\n*How it works:*\nLCA uses probabilities to model this. It says:\n\nEach class has a profile (e.g., 80% chance of answering “Yes” to Q1).\n\nEach person belongs to a class with a certain probability. Then it fits the best model using an algorithm called EM (Expectation Maximization).\n\n- **Strengths**: Handles multivariate categorical data, gives probabilities, supports statistical testing.\n- **Limitations**: Assumes conditional independence, slow for large data, risk of overfitting.\n- **Evaluation**: BIC (for model selection), ARI/NMI (if ground truth exists).\n\n### Model-Based (Bayesian): Dirichlet Process Mixture Models (DPMM)\nThis is like LCA’s more flexible, Bayesian cousin. Instead of deciding beforehand how many clusters you want (like “K = 3”), it lets the data tell you!\n\n*The big idea:*\n\nImagine a restaurant where each new customer either joins an existing table (cluster) or starts a new one.\n\nWhether they start a new table depends on how many people are already sitting and a concentration parameter (α).\n\nThis is called the Chinese Restaurant Process, and it’s the intuition behind DPMM.\n\n- **Strengths**: Automatically determines the number of clusters, flexible, supports soft clustering.\n- **Limitations**: Computationally intensive, requires setting/tuning hyperparameters.\n- **Evaluation**: Posterior probabilities, Predictive log-likelihood, ARI/NMI for known labels.\n\n::: {#955c89be .cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.mixture import BayesianGaussianMixture\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\n\n\n# Fit DPMM (upper bound on number of components, but fewer are used)\ndpgmm = BayesianGaussianMixture(\n    n_components=10,       # maximum number of components\n    covariance_type='full',\n    weight_concentration_prior_type='dirichlet_process',\n    weight_concentration_prior=0.5,  # lower value → more clusters\n    random_state=42\n)\n\ndpgmm.fit(X)\nlabels = dpgmm.predict(X)\nprobs = dpgmm.predict_proba(X)\n\n# Plot results\nplt.scatter(X[:, 0], X[:, 1], c=labels, cmap='Set2', s=40)\nplt.title('Dirichlet Process Gaussian Mixture Model (DPMM)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](MNL_files/figure-html/cell-4-output-1.png){width=558 height=431}\n:::\n:::\n\n\n#### What's Happening?\n- n_components=10: This sets the maximum number of clusters-not the actual number.\n\n- weight_concentration_prior: Lower values allow more small clusters. It's like tuning α in the Chinese Restaurant Process.\n\n\n### Hierarchical: ROCK and Gower’s Distance\nWhen clustering categorical data, we can’t use standard distances like Euclidean. So we need creative alternatives.\n\nROCK (Robust Clustering using Links)\nWhat makes ROCK special?\nIt’s designed specifically for categorical or binary data. Instead of relying just on similarity between two points, it looks at how many common neighbors they have.\n\nImagine:\n\nPerson A is similar to B and C.\n\nPerson D is also similar to B and C. Then A and D should probably be in the same group—even if they’re not directly that similar!\n\nIt’s like friend-of-a-friend logic. This method merges clusters that are strongly interconnected.\n\n- **ROCK** merges clusters based on the number of common neighbors above a similarity threshold.\n\nWhat is Gower’s Distance?\nIt’s a way to calculate distance that works on mixed data- numerical, categorical, and even ordinal.\n\nFor each variable:\n\n##### If it's numeric: take the normalized difference.\n##### If it's categorical: 0 if they match, 1 if they don’t.\n##### Then, average the distances. This gives a similarity that respects data types!\n\nHow it helps:\nOnce you compute the distance matrix using Gower, you can use any hierarchical clustering method, like AGNES, and plot a dendrogram.\n\n- **Gower + Hierarchical**: Use Gower’s distance (handles mixed types) with standard hierarchical clustering.\n\n- **Strengths**: ROCK is robust to noise; Gower allows mixed-type clustering.\n- **Limitations**: Requires threshold tuning (ROCK), slow for large datasets, not commonly implemented.\n- **Evaluation**: Silhouette with Gower/Hamming distance, visual inspection via dendrogram.\n\n### Distance-Based: K-Modes\n\nAn extension of K-Means for categorical data. It uses the mode (most common value) instead of the mean and mismatch count (Hamming distance) as the distance.\nWhy not k-means?\nBecause k-means computes centroids using averages, which doesn’t work for categories like “dog” or “banana.”\n\n#### K-Modes to the rescue:\n\nIt uses the mode (most frequent category) instead of the mean.\n\nIt measures distance by counting how many categories don’t match (like Hamming distance).\n\nExample: If A = [“red”, “large”, “yes”] and B = [“blue”, “large”, “no”]\nThen they differ in 2 out of 3 features → distance = 2\n\n- **Strengths**: Fast, interpretable, handles large categorical datasets.\n- **Limitations**: Needs K upfront, sensitive to initialization, can be affected by high-cardinality categories.\n- **Evaluation**: Cost function (total mismatches), Silhouette (with Hamming distance), ARI/NMI.\n\n### Distance-Based for Mixed Data: K-Prototypes\nThe real world isn't just numbers or categories—it’s both.\nEnter K-Prototypes, which combines:\n\nK-means (for numerical features)\n\nK-modes (for categorical features)\n\nHow it works:\n\nCalculates numerical distance (Euclidean) and categorical mismatch.\n\nAdds them together using a weight factor (γ) to balance them.\n\nCombines K-Means and K-Modes to handle both numeric and categorical data. Uses Euclidean distance for numerics, Hamming distance for categoricals.\n\n- **Strengths**: Handles mixed data, interpretable clusters.\n- **Limitations**: Requires tuning the weight parameter (gamma), needs K upfront.\n- **Evaluation**: Silhouette (with combined distance), ARI/NMI.\n\n---\n\n## Comparison Table\n| Algorithm | Category | Suitable Data Type |\n|----------|----------|---------------------|\n| GMM (e.g., Mclust) | Model-Based | Continuous |\n| AGNES (Agglomerative) | Hierarchical | Continuous |\n| DIANA (Divisive) | Hierarchical | Continuous |\n| K-Means | Distance-Based | Continuous |\n| LCA | Model-Based | Categorical |\n| DPMM | Model-Based | Categorical / Continuous |\n| ROCK | Hierarchical | Categorical |\n| Gower + Hierarchical | Hierarchical | Mixed / Categorical |\n| K-Modes | Distance-Based | Categorical |\n| K-Prototypes | Distance-Based | Mixed |\n\n---\n\n## Clustering Evaluation Metrics\n\n### Internal Metrics (No Ground Truth Needed)\n- **Silhouette Score**: Measures how well a point fits within its cluster. Ranges from -1 (bad) to +1 (good). Works with Euclidean, Hamming, or Gower distance.\n- **Dunn Index**: Ratio of minimum inter-cluster distance to maximum intra-cluster diameter. Higher is better.\n- **Davies-Bouldin Index (DBI)**: Measures average similarity between clusters. Lower is better.\n- **Calinski-Harabasz Index (CH)**: Ratio of between-cluster dispersion to within-cluster dispersion. Higher is better.\n- **Elbow Method**: Plot of total within-cluster sum of squares vs K. Look for an elbow point.\n- **BIC (Bayesian Information Criterion)**: Used in model-based clustering to balance model fit and complexity.\n\n### External Metrics (Need Ground Truth Labels)\n- **Adjusted Rand Index (ARI)**: Compares cluster assignments with true labels. Ranges from -1 to 1.\n- **Normalized Mutual Information (NMI)**: Measures mutual dependence between predicted clusters and true labels. Ranges from 0 to 1.\n- **Purity**: Fraction of dominant class members in each cluster. Simple but biased toward more clusters.\n\n---\n\n## Conclusion\nClustering is a powerful tool for uncovering hidden patterns in data. Different methods are suited to different data types:\n- Use **K-Means**, **GMM**, or **Hierarchical clustering** with Euclidean distance for continuous data.\n- Use **K-Modes**, **LCA**, **ROCK**, or **Gower-based methods** for categorical data.\n- Use **K-Prototypes** or **Gower** for mixed datasets.\n\nEvaluation is key. Use **internal metrics** like silhouette or BIC when ground truth isn’t available. Use **external metrics** like ARI and NMI when it is. Understanding both your data type and the assumptions of each algorithm is critical to choosing the right clustering technique.\n\n---\n\n",
    "supporting": [
      "MNL_files"
    ],
    "filters": [],
    "includes": {}
  }
}