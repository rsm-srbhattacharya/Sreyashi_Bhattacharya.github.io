{
  "hash": "47950301894043c0c84b83ccd59068ff",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Statistical Decision Theory: Classical and Bayesian Frameworks\"\nauthor: \"Sreyashi Bhattacharya\"\ndate: \"2025-04-10\"\nformat: html\ntoc: true\nnumber-sections: true\nexecute:\n  echo: true\n  warning: false\n  message: false\n---\n\n# Introduction\nStatistical decision theory provides a rigorous framework for making decisions under uncertainty using probability models and loss functions to quantify the cost of errors. In the 20th century, Abraham Wald formalized this framework by showing that classical problems like estimation and hypothesis testing are special cases of a general decision problem\n. In Waldâ€™s paradigm, each possible state of nature (parameter value) and action (decision) incurs a loss, and one seeks decision rules that perform optimally with respect to this loss.\n\n*Two major schools emerged:*\nClassical (Frequentist) Decision Theory: Treats the unknown parameter as fixed (though unknown) and seeks decisions with good frequentist properties (e.g. low risk for all or worst-case parameter values).\nBayesian Decision Theory: Treats the unknown parameter as random with a prior distribution, and seeks decisions that minimize Bayes risk (expected loss averaged over the prior).\nBoth frameworks share core conceptsâ€”loss functions, risk functions, and optimal decision rulesâ€”but differ in how they quantify uncertainty about parameters. This post reviews key ideas from each approach, including notions of minimax and Bayes rules, and provides examples with Python demonstrations. We will use graduate-level statistics context, citing foundational works (e.g. WaldÂ 1939, BergerÂ 1985) and illustrating concepts with practical examples and code.\n\n# Classical Decision Theory\nIn statistical decision theory, we think of **decisions as actions** we take based on data, and we evaluate how â€œgoodâ€ or â€œbadâ€ those actions are using **loss functions**. \n\nA **loss function**, denoted $L(Î¸, a)$, quantifies the cost of choosing action $a$ when the true (but unknown) state of nature is $Î¸$. For example, if you're estimating a parameter and guess too high or too low, the loss function penalizes you accordingly â€” perhaps proportionally to the square of the error.\n\nBut since we donâ€™t know the true value $Î¸$, we canâ€™t directly compute the loss â€” so instead, we consider the **expected loss**, averaged over the possible data outcomes we might observe. This gives us the **risk function**, $R(Î¸, Î´)$, which measures how well a decision rule $Î´$ performs if the true state is $Î¸$.\n\nLetâ€™s now formalize this setup.\n\n*Setup*: We have data $X$ with distribution depending on an unknown parameter $Î¸$ (often called the â€œstate of natureâ€). We must choose an action $a$ from an action space $ð’œ$ ((e.g. a real number if we are estimating Î¸, or a classification decision between two hypotheses or states). A loss function $L(Î¸,a)$ quantifies the penalty for taking action $a$ when the true parameter is $Î¸$. The statistician uses a decision rule $Î´(X)$ that maps observed data to an action. The quality of $Î´$ is measured by its risk function $R(Î¸, Î´) = \\mathbb{E}_Î¸[L(Î¸, Î´(X))]$, the expected loss under true parameter $Î¸$\n\nIntuitively, $R(Î¸, Î´)$ is the average loss weâ€™d incur if $Î¸$ were the true state and we used rule $Î´$. For example, if $L$ is squared error loss and $a(X)$ is an estimator of $Î¸$, then $R(Î¸, Î´)$ is the mean squared error (MSE) of that estimator at $Î¸$. Formally, for a given $Î¸$:\n$$\nR(Î¸,Î´)=âˆ«L(Î¸,Î´(x))dP_Î¸(x),\n$$\nwhere $P_Î¸$ is the probability distribution of $X$ when the parameter is $Î¸$â€‹\n\nA decision rule $Î´_1$ is said to dominate another rule $Î´_2$ if $R(Î¸, Î´_1) \\le R(Î¸, Î´_2)$ for all $Î¸$, and for some $Î¸$ the inequality is strictâ€‹\n\n. A rule that is not dominated by any other is called admissibleâ€‹\n. Goals: Since $Î¸$ is unknown, we cannot minimize $R(Î¸, Î´)$ for a specific $Î¸$ without more information. Classical decision theory considers several optimality criteria:\n\n## Minimax Criterion\n\nChoose the decision rule that minimizes the maximum risk across all $Î¸$. In other words, find $Î´^{*} = \\arg\\min_{Î´} \\max_{Î¸} R(Î¸,Î´)$\n. This criterion is conservative, treating nature as an adversary: it guarantees the smallest worst-case loss. Wald championed the minimax approach in early decision theory developmentâ€‹\n. For example, a minimax estimator is one that has the lowest possible worst-case MSE among all estimators.\n\n## Admissibility\n\nAs mentioned, an admissible rule is one that is not dominated by any other. An inadmissible rule is suboptimal since there exists another decision rule that performs at least as well for all $Î¸$ and better for some $Î¸$. A fundamental result is that minimax rules (under mild conditions) are often admissible, but not all admissible rules are minimax.\nOther criteria and methods (such as the Neymanâ€“Pearson lemma in hypothesis testing or invariance principles) also fit into the decision theory framework, but minimax is the canonical classical criterion. To illustrate, consider estimating a parameter $Î¸$ with squared error loss. The risk $R(Î¸, Î´)$ is the MSE. A minimax estimator would focus on controlling the worst-case MSE over all $Î¸$, while an admissible estimator simply needs to avoid being uniformly worse than another estimator. One famous result connecting to Bayesian methods is the complete class theorem, which states that under general conditions, any admissible decision rule is either a Bayes rule (minimizes Bayes risk for some prior) or a limit of Bayes rulesâ€‹\n. In other words, if a rule is not (approximately) Bayesian, one can find another rule that dominates itâ€‹\n. This bridges the gap between paradigms: from a frequentist perspective it justifies considering Bayes rules, since non-Bayes rules can often be improved.\n\n# Bayesian Decision Theory\n\nBayesian decision theory incorporates prior beliefs about $Î¸$ into the decision-making process. We assume a prior distribution $Ï€(Î¸)$ on the parameter. Upon observing data $x$, Bayesâ€™ theorem gives the posterior $Ï€(Î¸|x) âˆ Ï€(Î¸)Â·f(x|Î¸)$, where $f(x|Î¸)$ is the likelihood. The Bayesian analyst then chooses an action $a$ to minimize the posterior expected loss (also called posterior risk). Equivalently, Bayesâ€™ rule $Î´^{*}(x)$ is defined to minimize the Bayes risk, which is the expectation of the loss with respect to both the data and the prior for $Î¸$â€‹\n. Concretely, the Bayes risk of a rule $Î´$ is:\n$$\nÏ(Ï€,Î´)=âˆ«_Î˜ R(Î¸,Î´) Ï€(Î¸) dÎ¸ = E_{Î¸ âˆ¼ Ï€} [R(Î¸,Î´)]\n$$\nthe average risk under the prior. The Bayes optimal decision rule $Î´^{}$ is the one that minimizes this quantity (often computed by minimizing posterior expected loss for each observed $x$)â€‹\n. Equivalently, for each data outcome $x$, $Î´^{}(x)$ chooses the action $a$ that minimizes $\\mathbb{E}[L(Î¸,a) \\mid X=x]$ â€“ the loss averaged over the posterior for $Î¸$ given $X=x$â€‹\n. This rule is called the Bayes rule for prior $Ï€$. By design, it achieves the lowest prior-weighted average loss (the Bayes risk) among all decision rules.\n\n## Interpretation\n\nThe Bayesian approach yields a decision rule that is optimal on average according to the prior. One key consequence is that the Bayes rule is conditionally optimal for each realized dataset (since it picks the best action for the observed $x$), which can simplify decision derivationâ€‹\n. For example, a well-known result is that under 0â€“1 loss (classifying or deciding correctly vs incorrectly), the Bayes rule is to choose the hypothesis or class with highest posterior probability. This minimizes the probability of error. Another example: under squared error loss, the Bayes rule is to choose the posterior mean $E[Î¸ \\mid X=x]$ as the estimatorâ€‹\n, which minimizes the posterior expected squared error. (If the prior is flat/non-informative, the posterior mean often coincides with the classical unbiased estimator, bridging the approaches.)\n\n## Practical aspects\n\nBayesian decision theory requires specifying a prior $Ï€(Î¸)$, which can be based on past data or subjective belief. The choice of prior can influence the decision heavily, especially with limited data. However, as sample size grows, the influence of even a moderate prior diminishes (posterior concentrates around the true value by Bayes consistency). Bayesian methods shine in sequential or complex decision problems and naturally handle nuisance parameters by marginalizing them out via the prior/posterior. They also offer a straightforward way to incorporate asymmetric losses or multiple objectives by encoding them in $L(Î¸,a)$.\n\n## Admissibility connection\n\nAny Bayes rule with a proper prior is typically **admissible** â€” meaning no other rule performs strictly better across all parameter values. This is a powerful property: under broad conditions, using a Bayes rule automatically gives us a rule that cannot be uniformly improved upon.\n\nThis connects directly to a foundational result in decision theory known as the **complete class theorem**. The theorem states that â€” under general conditions â€” *every admissible rule is either a Bayes rule (for some prior), or the limit of a sequence of Bayes rules*. In other words, if you only consider Bayes rules, youâ€™re not missing out on any admissible rules. This justifies focusing on Bayes rules even from a frequentist perspective: any rule that isnâ€™t Bayesian (or close to it) can often be improved.\n\nInterestingly, the connection goes both ways. While Bayes rules minimize **average** risk under a prior, many **minimax rules** â€” which minimize **worst-case** risk â€” can also be interpreted as Bayes rules under a specially chosen prior. This prior is called a **least favorable prior** because it maximizes the Bayes risk among all priors. \n\nSo in practice, a minimax estimator or test can sometimes be **derived by imagining nature as an adversary**: we choose the worst-case prior (the one that leads to the highest risk), and then find the Bayes rule for that prior. The resulting rule ends up being minimax â€” that is, safest against the worst-case scenario.\n\nThus, while Bayesian and frequentist frameworks differ philosophically, their optimal decision rules often **intersect** when well-chosen priors are used.\n\n\n.\n\n# Examples\n\n## Example 1: Binary Classification (0â€“1 loss)\\\nIn this example, we simulate a **binary classification** problem under **0â€“1 loss**, which penalizes misclassification equally regardless of the direction or severity of the error.\n\nWe assume that:\n- Class 0 observations are drawn from a normal distribution centered at 0.\n- Class 1 observations are drawn from a normal distribution centered at 2.\n\nSince both classes are equally likely and have the same variance, the **Bayes classifier** assigns a new observation to class 1 if its value is greater than or equal to 1 â€” the midpoint between the two means. This decision rule minimizes the probability of error.\n\n::: {#a07caede .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\n\n# simulate data from each class\nnp.random.seed(0)\nN = 10000\nX0 = np.random.normal(0, 1, N)   # class 0 samples\nX1 = np.random.normal(2, 1, N)   # class 1 samples\n\n# Apply Bayes classifier: predict class 1 if x >= 1, else class 0\npred0_errors = np.sum(X0 >= 1)   # class 0 points misclassified as 1\npred1_errors = np.sum(X1 < 1)    # class 1 points misclassified as 0\nerror_rate = (pred0_errors + pred1_errors) / (2*N)\nprint(f\"Overall classification error rate: {error_rate:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOverall classification error rate: 0.152\n```\n:::\n:::\n\n\n## Example 2: Squared Error Risk â€“ Minimax vs Bayes\n\nThis example demonstrates how **risk functions** under **squared error loss** can be used to compare decision rules â€” in this case, two estimators for an unknown parameter $Î¸$:\n\n- The **sample mean**, which is an unbiased estimator and also the **Bayes estimator under a flat (non-informative) prior**.\n- A **biased estimator** that always overestimates the sample mean by 0.5.\n\nTo evaluate performance, we simulate data from normal distributions centered at various true values of $Î¸$, compute the squared error for each estimator, and average over many simulations. This gives us the **risk function** $R(Î¸, Î´)$ for each estimator, plotted over a range of $Î¸$ values.\n\nThe result shows that:\n- The sample mean generally has lower risk across all $Î¸$.\n- The biased estimator incurs consistently higher risk, especially when the bias pushes it further away from the true value.\n\nThis example reinforces the idea that **introducing bias can increase risk**, and also illustrates how **risk functions** are central to comparing estimators under both minimax and Bayes decision-theoretic criteria.\n\n::: {#074aed0d .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# True theta values to evaluate over\ntheta_vals = np.linspace(-2, 2, 100)\nsample_size = 20\nbiased_shift = 0.5\n\n# Define two estimators: unbiased sample mean vs biased estimator\ndef sample_mean(X): return np.mean(X)\ndef biased_estimator(X): return np.mean(X) + biased_shift\n\n# Risk is MSE: average over many simulations\ndef compute_risk(estimator, theta_vals, n=sample_size, sims=5000):\n    risks = []\n    for theta in theta_vals:\n        losses = []\n        for _ in range(sims):\n            X = np.random.normal(theta, 1, n)\n            est = estimator(X)\n            losses.append((theta - est)**2)\n        risks.append(np.mean(losses))\n    return risks\n\nrisk_mean = compute_risk(sample_mean, theta_vals)\nrisk_biased = compute_risk(biased_estimator, theta_vals)\n\n# Plot\nplt.figure(figsize=(8, 5))\nplt.plot(theta_vals, risk_mean, label='Sample Mean (Bayes under flat prior)')\nplt.plot(theta_vals, risk_biased, label='Biased Estimator (+0.5)', linestyle='--')\nplt.xlabel(\"True Î¸\")\nplt.ylabel(\"Risk (MSE)\")\nplt.title(\"Risk Functions of Two Estimators\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Decision Theory_files/figure-html/cell-3-output-1.png){width=757 height=468}\n:::\n:::\n\n\n## Conclusion\n\nStatistical decision theory unifies estimation, testing, and prediction under a common framework of decisions and losses. The classical approach focuses on guarantees and performance for all possible parameter values, leading to methods like minimax rules that safeguard against the worst-case . The Bayesian approach incorporates prior information to optimize performance on average according to that prior, yielding intuitively appealing rules like â€œpick the most probable hypothesisâ€. In practice, these approaches are often complementary: Bayesian methods provide a flexible way to include prior knowledge and handle complex problems, while frequentist criteria ensure robustness and avoid reliance on subjective inputs. Modern statistical practice often blends the twoâ€”for example, using Bayesian-inspired techniques with carefully chosen priors that yield frequentist guarantees. \n\n\n\nFor further reading, a comprehensive reference on decision theory is James Bergerâ€™s *Statistical Decision Theory and Bayesian Analysis (1985)* , which covers both classical and Bayesian perspectives in depth. Another classic text is Morris DeGrootâ€™s *Optimal Statistical Decisions (1970)* , which provides a thorough treatment of decision theory concepts and criteria. These works, building on Waldâ€™s , demonstrate how principles of loss and risk inform much of modern statistical methodologyâ€”from designing estimators and tests to understanding machine learning algorithms as loss-minimization decision rules. By understanding both the classical and Bayesian decision frameworks, one gains a deeper insight into why statistical procedures are formulated the way they are, and how to choose or tailor them for specific decision problems in research and applications.\n\n",
    "supporting": [
      "Decision Theory_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}